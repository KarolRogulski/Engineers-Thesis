{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engineer's Thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows only VGG19 training for preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow loggin disabled\n",
    "import datetime, logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow \n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = ''\n",
    "data_path = path + 'data/'\n",
    "model_path = path + 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "df = tfds.load(name=\"stanford_dogs\", split='train+test')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conf and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "img_size = 224\n",
    "n_classes = 120\n",
    "seed = 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "# Function counting classes distribution\n",
    "def count_class_distribution(ds):\n",
    "  count = np.zeros(shape=n_classes)\n",
    "  c = 0\n",
    "  for example in ds.take(-1):\n",
    "    label = example[\"label\"]\n",
    "    count[tf.keras.backend.get_value(label)] +=1\n",
    "    c+=1\n",
    "  return count, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count, n_examples = count_class_distribution(df)\n",
    "\n",
    "# Count distribution matrix\n",
    "test_count = class_count\n",
    "distribution = test_count/n_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = []\n",
    "\n",
    "# Create subsets, one for every class\n",
    "for i in range(0, (n_classes)):\n",
    "  dataset = df.filter(lambda fd: fd['label'] == i)\n",
    "  splits.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_size = 1000\n",
    "validation_split = 0.8\n",
    "\n",
    "# Create training, validation and test set\n",
    "def get_smaller_ds(ds, splits, size):\n",
    "  train_ds = df.take(0)\n",
    "  val_ds = df.take(0)\n",
    "  test_ds = df.take(0)\n",
    "\n",
    "  for d, s in zip(distribution, splits):\n",
    "    n_test_ds = int(d * test_ds_size)\n",
    "    n_train_ds = int(d * (size * validation_split))\n",
    "    n_val_ds = int(d * (size * (1 - validation_split)))\n",
    "    \n",
    "    # test\n",
    "    test_result = s.take(n_test_ds)\n",
    "    test_ds = test_ds.concatenate(test_result)\n",
    "\n",
    "    #train\n",
    "    train_result = s.skip(n_test_ds)\n",
    "    train_result = train_result.take(n_train_ds)\n",
    "    train_ds = train_ds.concatenate(train_result)\n",
    "\n",
    "    #val\n",
    "    val_skip = n_test_ds + n_train_ds\n",
    "    val_result = s.skip(val_skip)\n",
    "    val_result = val_result.take(n_val_ds)\n",
    "    val_ds = val_ds.concatenate(val_result)\n",
    "\n",
    "  return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds_row):\n",
    "    # Image conversion int->float + resizing\n",
    "    #image = tf.image.convert_image_dtype(ds_row['image'], dtype=tf.float32)\n",
    "    image = tf.image.resize(ds_row['image'], (img_size, img_size), method='nearest')\n",
    "  \n",
    "    # Onehot encoding labels\n",
    "    label = tf.one_hot(ds_row['label'], n_classes)\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(dataset, batch_size):\n",
    "    ds = dataset.map(preprocess, num_parallel_calls=4)\n",
    "    ds = ds.shuffle(buffer_size=(n_examples*2), seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Flatten\n",
    "from tensorflow.keras.layers import Dropout, Lambda, BatchNormalization\n",
    "from tensorflow.keras.layers import Rescaling, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation layer\n",
    "augmentation = models.Sequential(\n",
    "    [\n",
    "        layers.RandomRotation(factor=0.15),\n",
    "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "        layers.RandomFlip(),\n",
    "        layers.RandomContrast(factor=0.1),\n",
    "    ],\n",
    "    name=\"augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Function that plots metrics after training\n",
    "def plot_metrics(name, history, epochs, lr):  \n",
    "  history_accuracy = {\n",
    "      'accuracy' : history.history['accuracy'],\n",
    "      'val_accuracy' : history.history['val_accuracy']\n",
    "  }\n",
    "  history_loss = {\n",
    "      'loss' : history.history['loss'],\n",
    "      'val_loss' : history.history['val_loss']\n",
    "  }\n",
    "\n",
    "  plt.plot(history.history['accuracy'])\n",
    "  plt.plot(history.history['val_accuracy'])\n",
    "  plt.grid(True)\n",
    "  plt.title(name + '     Accuracy metrics     Learning rate: ' + str(lr))\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend(['train', 'val'], loc='upper left')\n",
    "  plt.show()\n",
    "  # -----\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.grid(True)\n",
    "  plt.title(name + '     Loss metrics     Learning rate: ' + str(lr))\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend(['train', 'val'], loc='upper left')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of callbacks\n",
    "def list_of_callbacks(name, learning_rate):\n",
    "  filepath = model_path + name  + 'lr' + str(learning_rate)\n",
    "\n",
    "    # Save model callback\n",
    "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=filepath,\n",
    "      save_weights_only=False,\n",
    "      monitor='val_accuracy',\n",
    "      verbose=0,\n",
    "      save_freq='epoch',\n",
    "      save_best_only=False)\n",
    "  \n",
    "  # Save history callback\n",
    "  history_logger=tf.keras.callbacks.CSVLogger(filepath + '.csv', \n",
    "                                              separator=\",\", \n",
    "                                              append=True)\n",
    "  \n",
    "  # Early stopping if no progress or bad at start\n",
    "  early = tf.keras.callbacks.EarlyStopping(monitor='accuracy', \n",
    "                                           min_delta=0.01, \n",
    "                                           patience=4, \n",
    "                                           verbose=0, \n",
    "                                           mode='auto', \n",
    "                                          #  baseline=0.7, \n",
    "                                           restore_best_weights=False)\n",
    "  \n",
    "  logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, \n",
    "                                                        histogram_freq=1)\n",
    "\n",
    "  return [history_logger, tensorboard_callback, model_checkpoint_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Compile model -> train model -> plot metrics after training\n",
    "def test_model(name, model, train_data, valid_data, learning_rate):\n",
    "  print(\"Model name:  \", name, \" learning rate:  \", str(learning_rate))\n",
    "\n",
    "  model.compile(optimizer=Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  history = model.fit(train_data,\n",
    "                    validation_data=valid_data,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=list_of_callbacks(name, learning_rate))\n",
    "  \n",
    "  plot_metrics(name, history, epochs, learning_rate)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "def eval(name, model, train_batches, val_batches, test_batches, lr_val):\n",
    "  model_array = np.empty(shape=(len(lr_val), 2))\n",
    "  # import initial weights (prevents starting training with trained weights)\n",
    "  weights = model.get_weights()\n",
    "  \n",
    "  # train for different learning rates \n",
    "  count = 0\n",
    "  for i in lr_val:\n",
    "    model.set_weights(weights)\n",
    "    m = test_model(name, model, train_batches, val_batches, i)\n",
    "    r = m.evaluate(test_batches)\n",
    "    #Debug\n",
    "    print(r)\n",
    "    model_array[count] = r\n",
    "    count += 1\n",
    "    nam = name + str(i) + '.csv'\n",
    "    np.savetxt(nam, model_array, delimiter=',')\n",
    "\n",
    "  return model_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function which creates model with preprocessing layer \n",
    "def create_model(base_model, preprocess):\n",
    "  base_model.trainable = False\n",
    "\n",
    "  preprocess_layer = Lambda(preprocess, \n",
    "                          name='preprocessing', \n",
    "                          input_shape=(img_size, img_size, 3))\n",
    "\n",
    "  model = models.Sequential(\n",
    "    [\n",
    "     keras.Input(shape=(img_size, img_size, 3)),\n",
    "     augmentation,\n",
    "     preprocess_layer,\n",
    "\n",
    "     base_model,\n",
    "     GlobalAveragePooling2D(),\n",
    "     Dense(n_classes, activation = 'softmax', name='output')\n",
    "    ]\n",
    "  )\n",
    "  model.summary()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "epochs = 25\n",
    "lr = 0.0005\n",
    "train_ds_size = 4000\n",
    "\n",
    "train_data, val_data, test_data = get_smaller_ds(df, splits, train_ds_size)\n",
    "\n",
    "train_batches = prepare(train_data, batch_size=batch_size)\n",
    "val_batches = prepare(val_data, batch_size=batch_size)\n",
    "test_batches = prepare(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## VG19 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide each training for separate cell for safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import preprocess_input, VGG19\n",
    "\n",
    "row_XC = pd.DataFrame(columns=['loss','accuracy', 'training_time', 'test_time'])\n",
    "name = (\"VGG19\" + str(train_ds_size) + '.csv')\n",
    "row_XC.to_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG19(weights = 'imagenet', \n",
    "                        include_top = False, \n",
    "                        input_shape = (img_size, img_size, 3))\n",
    "\n",
    "model = create_model(base_model, \n",
    "                        tensorflow.keras.applications.vgg19.preprocess_input)\n",
    "  \n",
    "start_t = time.time()\n",
    "m = test_model(name, model, train_batches, val_batches, lr)\n",
    "end_t = time.time()\n",
    "\n",
    "start_e = time.time()\n",
    "r = m.evaluate(test_batches)\n",
    "end_e = time.time()\n",
    "\n",
    "row_XC = row_XC.append(pd.DataFrame([[r[0], r[1], (end_t-start_t), (end_e-start_e)]], \n",
    "                    columns=['loss','accuracy', 'training_time', 'test_time']))\n",
    "\n",
    "r = pd.read_csv(name)\n",
    "\n",
    "r = pd.concat([r,row_XC])\n",
    "r.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG19(weights = 'imagenet', \n",
    "                        include_top = False, \n",
    "                        input_shape = (img_size, img_size, 3))\n",
    "\n",
    "model = create_model(base_model, \n",
    "                        tensorflow.keras.applications.vgg19.preprocess_input)\n",
    "  \n",
    "start_t = time.time()\n",
    "m = test_model(name, model, train_batches, val_batches, lr)\n",
    "end_t = time.time()\n",
    "\n",
    "start_e = time.time()\n",
    "r = m.evaluate(test_batches)\n",
    "end_e = time.time()\n",
    "\n",
    "row_XC = row_XC.append(pd.DataFrame([[r[0], r[1], (end_t-start_t), (end_e-start_e)]], \n",
    "                    columns=['loss','accuracy', 'training_time', 'test_time']))\n",
    "\n",
    "r = pd.read_csv(name)\n",
    "\n",
    "r = pd.concat([r,row_XC])\n",
    "r.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## VGG19 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_size = 8000\n",
    "\n",
    "train_data, val_data, test_data = get_smaller_ds(df, splits, train_ds_size)\n",
    "\n",
    "train_batches = prepare(train_data, batch_size=batch_size)\n",
    "val_batches = prepare(val_data, batch_size=batch_size)\n",
    "test_batches = prepare(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import preprocess_input, VGG19\n",
    "\n",
    "row_XC = pd.DataFrame(columns=['loss','accuracy', 'training_time', 'test_time'])\n",
    "name = (\"VGG19\" + str(train_ds_size) + '.csv')\n",
    "row_XC.to_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG19(weights = 'imagenet', \n",
    "                        include_top = False, \n",
    "                        input_shape = (img_size, img_size, 3))\n",
    "\n",
    "model = create_model(base_model, \n",
    "                        tensorflow.keras.applications.vgg19.preprocess_input)\n",
    "  \n",
    "start_t = time.time()\n",
    "m = test_model(name, model, train_batches, val_batches, lr)\n",
    "end_t = time.time()\n",
    "\n",
    "start_e = time.time()\n",
    "r = m.evaluate(test_batches)\n",
    "end_e = time.time()\n",
    "\n",
    "row_XC = row_XC.append(pd.DataFrame([[r[0], r[1], (end_t-start_t), (end_e-start_e)]], \n",
    "                    columns=['loss','accuracy', 'training_time', 'test_time']))\n",
    "\n",
    "r = pd.read_csv(name)\n",
    "\n",
    "r = pd.concat([r,row_XC])\n",
    "r.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG19(weights = 'imagenet', \n",
    "                        include_top = False, \n",
    "                        input_shape = (img_size, img_size, 3))\n",
    "\n",
    "model = create_model(base_model, \n",
    "                        tensorflow.keras.applications.vgg19.preprocess_input)\n",
    "  \n",
    "start_t = time.time()\n",
    "m = test_model(name, model, train_batches, val_batches, lr)\n",
    "end_t = time.time()\n",
    "\n",
    "start_e = time.time()\n",
    "r = m.evaluate(test_batches)\n",
    "end_e = time.time()\n",
    "\n",
    "row_XC = row_XC.append(pd.DataFrame([[r[0], r[1], (end_t-start_t), (end_e-start_e)]], \n",
    "                    columns=['loss','accuracy', 'training_time', 'test_time']))\n",
    "\n",
    "r = pd.read_csv(name)\n",
    "\n",
    "r = pd.concat([r,row_XC])\n",
    "r.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## VGG19 12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_size = 12000\n",
    "\n",
    "train_data, val_data, test_data = get_smaller_ds(df, splits, train_ds_size)\n",
    "\n",
    "train_batches = prepare(train_data, batch_size=batch_size)\n",
    "val_batches = prepare(val_data, batch_size=batch_size)\n",
    "test_batches = prepare(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import preprocess_input, VGG19\n",
    "\n",
    "row_XC = pd.DataFrame(columns=['loss','accuracy', 'training_time', 'test_time'])\n",
    "name = (\"VGG19\" + str(train_ds_size) + '.csv')\n",
    "row_XC.to_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG19(weights = 'imagenet', \n",
    "                        include_top = False, \n",
    "                        input_shape = (img_size, img_size, 3))\n",
    "\n",
    "model = create_model(base_model, \n",
    "                        tensorflow.keras.applications.vgg19.preprocess_input)\n",
    "  \n",
    "start_t = time.time()\n",
    "m = test_model(name, model, train_batches, val_batches, lr)\n",
    "end_t = time.time()\n",
    "\n",
    "start_e = time.time()\n",
    "r = m.evaluate(test_batches)\n",
    "end_e = time.time()\n",
    "\n",
    "row_XC = row_XC.append(pd.DataFrame([[r[0], r[1], (end_t-start_t), (end_e-start_e)]], \n",
    "                    columns=['loss','accuracy', 'training_time', 'test_time']))\n",
    "\n",
    "r = pd.read_csv(name)\n",
    "\n",
    "r = pd.concat([r,row_XC])\n",
    "r.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG19(weights = 'imagenet', \n",
    "                        include_top = False, \n",
    "                        input_shape = (img_size, img_size, 3))\n",
    "\n",
    "model = create_model(base_model, \n",
    "                        tensorflow.keras.applications.vgg19.preprocess_input)\n",
    "  \n",
    "start_t = time.time()\n",
    "m = test_model(name, model, train_batches, val_batches, lr)\n",
    "end_t = time.time()\n",
    "\n",
    "start_e = time.time()\n",
    "r = m.evaluate(test_batches)\n",
    "end_e = time.time()\n",
    "\n",
    "row_XC = row_XC.append(pd.DataFrame([[r[0], r[1], (end_t-start_t), (end_e-start_e)]], \n",
    "                    columns=['loss','accuracy', 'training_time', 'test_time']))\n",
    "\n",
    "r = pd.read_csv(name)\n",
    "\n",
    "r = pd.concat([r,row_XC])\n",
    "r.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_size = n_examples\n",
    "\n",
    "train_data, val_data, test_data = get_smaller_ds(df, splits, train_ds_size)\n",
    "\n",
    "train_batches = prepare(train_data, batch_size=batch_size)\n",
    "val_batches = prepare(val_data, batch_size=batch_size)\n",
    "test_batches = prepare(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import preprocess_input, VGG19\n",
    "\n",
    "row_XC = pd.DataFrame(columns=['loss','accuracy', 'training_time', 'test_time'])\n",
    "name = (\"VGG19\" + str(train_ds_size) + '.csv')\n",
    "row_XC.to_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG19(weights = 'imagenet', \n",
    "                        include_top = False, \n",
    "                        input_shape = (img_size, img_size, 3))\n",
    "\n",
    "model = create_model(base_model, \n",
    "                        tensorflow.keras.applications.vgg19.preprocess_input)\n",
    "  \n",
    "start_t = time.time()\n",
    "m = test_model(name, model, train_batches, val_batches, lr)\n",
    "end_t = time.time()\n",
    "\n",
    "start_e = time.time()\n",
    "r = m.evaluate(test_batches)\n",
    "end_e = time.time()\n",
    "\n",
    "row_XC = row_XC.append(pd.DataFrame([[r[0], r[1], (end_t-start_t), (end_e-start_e)]], \n",
    "                    columns=['loss','accuracy', 'training_time', 'test_time']))\n",
    "\n",
    "r = pd.read_csv(name)\n",
    "\n",
    "r = pd.concat([r,row_XC])\n",
    "r.to_csv(name, index=False)\n",
    "# 11 epochs gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG19(weights = 'imagenet', \n",
    "                        include_top = False, \n",
    "                        input_shape = (img_size, img_size, 3))\n",
    "\n",
    "model = create_model(base_model, \n",
    "                        tensorflow.keras.applications.vgg19.preprocess_input)\n",
    "  \n",
    "start_t = time.time()\n",
    "m = test_model(name, model, train_batches, val_batches, lr)\n",
    "end_t = time.time()\n",
    "\n",
    "start_e = time.time()\n",
    "r = m.evaluate(test_batches)\n",
    "end_e = time.time()\n",
    "\n",
    "row_XC = row_XC.append(pd.DataFrame([[r[0], r[1], (end_t-start_t), (end_e-start_e)]], \n",
    "                    columns=['loss','accuracy', 'training_time', 'test_time']))\n",
    "\n",
    "r = pd.read_csv(name)\n",
    "\n",
    "r = pd.concat([r,row_XC])\n",
    "r.to_csv(name, index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
